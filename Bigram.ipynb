{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "o0VvgY_KiT0O",
    "outputId": "c46a76ac-9141-470a-9d0f-fc3e88109a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shari\\AppData\\Roaming\\jupyter\\runtime\\kernel-233e6658-c27a-4189-81e3-01ae979831b9.json\n",
      "unigram_count of the 3676\n",
      "unique_unigram_count 7602\n",
      "unigram_prob of the 0.053479203340267976\n",
      "total_tokens 68737\n",
      "unique bigram_count 28268\n",
      "total_bigrams 66517\n",
      "\n",
      "the:0.053479203340267976 ('the', 'standard'):0.0008161044613710555 ('standard', 'turbo'):0.2 ('turbo', 'engine'):0 ('engine', 'is'):0 ('is', 'hard'):0 ('hard', 'to'):0.75 ('to', 'work'):0.004513217279174726 \n",
      "No smoothed prob: 0.0\n",
      "\n",
      "the:0.053479203340267976 ('the', 'standard'):0.0003546728143287817 ('standard', 'turbo'):0.00039411455596426696 ('turbo', 'engine'):0.0001315097317201473 ('engine', 'is'):0.000131250820317627 ('is', 'hard'):0.00012423903590508137 ('hard', 'to'):0.0005259006047856955 ('to', 'work'):0.0008740303725554463 \n",
      "Laplace smoothed prob: 7.368574707097542e-27\n",
      "\n",
      "the:0.053479203340267976 ('the', 'standard'):4.520905085529811e-05 ('standard', 'turbo'):1.2824430746649818e-05 ('turbo', 'engine'):0.2483575627283251 ('engine', 'is'):0.2483575627283251 ('is', 'hard'):0.2483575627283251 ('hard', 'to'):4.520905085529811e-05 ('to', 'work'):0.00010794966406900566 \n",
      "Good Turing prob: 2.3180738471938815e-21\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_words(filename):\n",
    "    file1 = open(filename,\"r\")\n",
    "    tokenslist = []\n",
    "    wordslist = []\n",
    "    for line in file1.readlines():\n",
    "        tokenslist.append(line.split())\n",
    "        wordslist.append([token.split('_')[0].lower() for token in line.split()])\n",
    "        \n",
    "    return tokenslist, wordslist\n",
    "\n",
    "def get_unigram_count(wordslist):\n",
    "    unigram_count = {}\n",
    "    total_tokens = 0\n",
    "    for line in wordslist:\n",
    "        for word in line:\n",
    "            if word not in unigram_count:\n",
    "                unigram_count[word] = 1\n",
    "            else:\n",
    "                unigram_count[word] += 1\n",
    "        total_tokens += len(line)\n",
    "        \n",
    "    return unigram_count, total_tokens\n",
    "\n",
    "def get_unigram_prob(unigram_count, total_tokens):\n",
    "    unigram_prob = {}\n",
    "    for key in unigram_count:\n",
    "        unigram_prob[key] = unigram_count[key]/total_tokens\n",
    "    \n",
    "    return unigram_prob\n",
    "\n",
    "def get_bigram_count(wordslist):\n",
    "    bigram_count = {}\n",
    "    for line in wordslist:\n",
    "        for i in range(1, len(line)):\n",
    "            if (line[i-1], line[i]) not in bigram_count:\n",
    "                bigram_count[(line[i-1], line[i])] = 1\n",
    "            else:\n",
    "                bigram_count[(line[i-1], line[i])] += 1\n",
    "    \n",
    "    return bigram_count\n",
    "\n",
    "def get_bigram_prob(bigram_count, unigram_count, wordslist):\n",
    "    bigram_prob = {}\n",
    "    for key in bigram_count:\n",
    "        bigram_prob[key] = bigram_count[key]/unigram_count[key[0]]\n",
    "    total_bigrams = sum([len(line)-1 for line in wordslist])\n",
    "    \n",
    "    return bigram_prob, total_bigrams\n",
    "\n",
    "def get_bigram_laplace_prob(bigram_count, unigram_count, word_count_V):\n",
    "    bigram_laplace_prob = {}\n",
    "    for key in bigram_count:\n",
    "        bigram_laplace_prob[key] = (bigram_count[key]+1)/(unigram_count[key[0]]+word_count_V)\n",
    "        \n",
    "    return bigram_laplace_prob\n",
    "\n",
    "def get_no_smooth_prob(sentence, unigram_prob, bigram_prob):\n",
    "    no_smooth_prob = 1\n",
    "    for i in range(len(sentence)):\n",
    "        if i==0:\n",
    "            if sentence[i] in unigram_prob:\n",
    "                no_smooth_prob *= unigram_prob[sentence[i]]\n",
    "                print(sentence[i]+':'+str(unigram_prob[sentence[i]]), end=' ')\n",
    "            else:\n",
    "                no_smooth_prob = 0\n",
    "                print(sentence[i]+':'+str(0), end=' ')\n",
    "        else:\n",
    "            if (sentence[i-1], sentence[i]) in bigram_prob:\n",
    "                no_smooth_prob *= bigram_prob[(sentence[i-1], sentence[i])]\n",
    "                print(str((sentence[i-1], sentence[i]))+':'+str(bigram_prob[(sentence[i-1], sentence[i])]), end=' ')\n",
    "            else:\n",
    "                no_smooth_prob = 0\n",
    "                print(str((sentence[i-1], sentence[i]))+':'+str(0), end=' ')\n",
    "    print()\n",
    "    return no_smooth_prob\n",
    "\n",
    "def get_laplace_smooth_prob(sentence, unigram_prob, bigram_laplace_prob, unigram_count, word_count_V):\n",
    "    laplace_smooth_prob = 1\n",
    "    for i in range(len(sentence)):\n",
    "        if i==0:\n",
    "            if sentence[i] in unigram_prob:\n",
    "                laplace_smooth_prob *= unigram_prob[sentence[i]]\n",
    "                print(sentence[i]+':'+str(unigram_prob[sentence[i]]), end=' ')\n",
    "            else:\n",
    "                laplace_smooth_prob = 0\n",
    "                print(sentence[i]+':'+str(0), end=' ')\n",
    "        else:\n",
    "            if (sentence[i-1], sentence[i]) in bigram_laplace_prob:\n",
    "                laplace_smooth_prob *= bigram_laplace_prob[(sentence[i-1], sentence[i])]\n",
    "                print(str((sentence[i-1], sentence[i]))+':'+str(bigram_laplace_prob[(sentence[i-1], sentence[i])]), end=' ')\n",
    "            else:\n",
    "                laplace_smooth_prob *=  1/(unigram_count[sentence[i-1]]+word_count_V)\n",
    "                print(str((sentence[i-1], sentence[i]))+':'+str(1/(unigram_count[sentence[i-1]]+word_count_V)), end=' ')\n",
    "    print()\n",
    "    return laplace_smooth_prob\n",
    "\n",
    "def get_N_c(bigram_count):\n",
    "    N_c={}\n",
    "    for key in bigram_count:\n",
    "        if bigram_count[key] in N_c:\n",
    "            N_c[bigram_count[key]].append(key)\n",
    "        else:\n",
    "            N_c[bigram_count[key]] = [key]\n",
    "            \n",
    "    return N_c\n",
    "\n",
    "def get_cstar_pstar(N_c, total_bigrams):\n",
    "    c_star = {}\n",
    "    p_star = {}\n",
    "    p_star[0] = len(N_c[1])/total_bigrams\n",
    "    for key in N_c:\n",
    "        if key+1 in N_c:\n",
    "            c_star[key] = (key+1)*len(N_c[key+1])/len(N_c[key])\n",
    "        else:\n",
    "            c_star[key] = 0\n",
    "        p_star[key] = c_star[key]/total_bigrams\n",
    "        \n",
    "    return c_star, p_star\n",
    "\n",
    "def get_GT_smooth_prob(sentence, unigram_prob, bigram_count, p_star):\n",
    "    GT_smooth_prob = 1\n",
    "    for i in range(len(sentence)):\n",
    "        if i==0:\n",
    "            if sentence[i] in unigram_prob:\n",
    "                GT_smooth_prob *= unigram_prob[sentence[i]]\n",
    "                print(sentence[i]+':'+str(unigram_prob[sentence[i]]), end=' ')\n",
    "            else:\n",
    "                GT_smooth_prob = 0\n",
    "                print(sentence[i]+':'+str(0), end=' ')\n",
    "        else:\n",
    "            if (sentence[i-1], sentence[i]) in bigram_count:\n",
    "                GT_smooth_prob *= p_star[bigram_count[(sentence[i-1], sentence[i])]]\n",
    "                print(str((sentence[i-1], sentence[i]))+':'+str(p_star[bigram_count[(sentence[i-1], sentence[i])]]), end=' ')\n",
    "            else:\n",
    "                GT_smooth_prob *=  p_star[0]\n",
    "                print(str((sentence[i-1], sentence[i]))+':'+str(p_star[0]), end=' ')\n",
    "    print()\n",
    "    return GT_smooth_prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    print((sys.argv[2]))\n",
    "    tokenslist, wordslist = get_tokens_words(\"NLP6320_POSTaggedTrainingSet-Windows.txt\")\n",
    "    unigram_count, total_tokens = get_unigram_count(wordslist)\n",
    "    print('unigram_count of the', unigram_count['the'])\n",
    "    print('unique_unigram_count', len(unigram_count))\n",
    "    unigram_prob = get_unigram_prob(unigram_count, total_tokens)\n",
    "    print('unigram_prob of the', unigram_prob['the'])\n",
    "    print('total_tokens', total_tokens)\n",
    "    bigram_count = get_bigram_count(wordslist)\n",
    "    print('unique bigram_count',len(bigram_count))\n",
    "    bigram_prob, total_bigrams = get_bigram_prob(bigram_count, unigram_count, wordslist)\n",
    "    print('total_bigrams',total_bigrams)\n",
    "    word_count_V = len(unigram_count)\n",
    "    bigram_laplace_prob = get_bigram_laplace_prob(bigram_count, unigram_count, word_count_V)\n",
    "    sentence = 'The standard Turbo engine is hard to work'.lower().split()\n",
    "    print()\n",
    "    no_smooth_prob = get_no_smooth_prob(sentence, unigram_prob, bigram_prob)\n",
    "    print('No smoothed prob:',no_smooth_prob)\n",
    "    print()\n",
    "    laplace_smooth_prob = get_laplace_smooth_prob(sentence, unigram_prob, bigram_laplace_prob, unigram_count, word_count_V)\n",
    "    print('Laplace smoothed prob:',laplace_smooth_prob)\n",
    "    N_c = get_N_c(bigram_count)\n",
    "    possible_bigrams = word_count_V**2\n",
    "    c_star, p_star = get_cstar_pstar(N_c, total_bigrams)\n",
    "    print()\n",
    "    GT_smooth_prob = get_GT_smooth_prob(sentence, unigram_prob, bigram_count, p_star)\n",
    "    print('Good Turing prob:',GT_smooth_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3j9HiQxvaIe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_count 7\n",
      "N_c 184\n",
      "N_c 205\n",
      "N_c 16520\n",
      "1813\n",
      "1363\n",
      "4.520905085529811e-05\n",
      "3.0071704357418643\n"
     ]
    }
   ],
   "source": [
    "print('bigram_count',bigram_count[('to','work')])\n",
    "print('N_c', len(N_c[8]))\n",
    "print('N_c', len(N_c[7]))\n",
    "print('N_c', len(N_c[1]))\n",
    "print(len(N_c[3]))\n",
    "print(len(N_c[4]))\n",
    "print(p_star[3])\n",
    "print(c_star[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str or None, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-397fa2d47d9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m\"A_x b_y\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be str or None, not list"
     ]
    }
   ],
   "source": [
    "\"A_x b_y\".split(['_',' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bigram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
